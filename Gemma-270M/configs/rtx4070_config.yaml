data:
  data_dir: data
  dataset_name: roneneldan/TinyStories
  force_reprocess: false
  num_proc: 8
  num_workers: 0
  pin_memory: true
  tokenizer_name: gpt2
  train_path: data/train.bin
  val_path: data/validation.bin
description: 'RTX 4070 optimized config: batch=8, seq=256, reduced parameters for 8GB VRAM'
model:
  context_length: 16384
  dtype: torch.float32
  emb_dim: 768    # Reduced from 896
  head_dim: 96    # Reduced from 112
  hidden_dim: 3072  # Reduced from 3584
  layer_types:
    - sliding_attention
    - sliding_attention
    - sliding_attention
    - sliding_attention
    - sliding_attention
    - full_attention
    - sliding_attention
    - sliding_attention
    - sliding_attention
    - sliding_attention
    - sliding_attention
    - full_attention
    - sliding_attention
    - sliding_attention
    - sliding_attention
    - full_attention
  n_heads: 8
  n_kv_groups: 1
  n_layers: 16  # Reduced from 22
  qk_norm: true
  query_pre_attn_scalar: 256
  rope_base: 1000000.0
  rope_local_base: 10000.0
  sliding_window: 512
  vocab_size: 50257
name: gemma-270m-rtx4070
tags:
- gemma-270m
- rtx4070
- memory-efficient
- mid-range-gpu
training:
  batch_size: 8      # Reduced from 16
  beta1: 0.9
  beta2: 0.95
  block_size: 256    # Reduced from 384
  compile_model: false
  device: cuda
  dtype: float32
  early_stopping_patience: null
  eps: 1.0e-09
  eval_interval: 500
  eval_iters: 200    # Reduced for memory
  grad_clip: 1.0
  gradient_accumulation_steps: 32  # Reduced from 64
  learning_rate: 0.0001
  log_interval: 10
  max_iters: 50000   # Reduced training steps for testing
  min_lr: 5.0e-05
  output_dir: checkpoints
  resume_from: null
  save_best_only: true
  save_interval: 1000
  warmup_steps: 500   # Reduced
  weight_decay: 0.1
